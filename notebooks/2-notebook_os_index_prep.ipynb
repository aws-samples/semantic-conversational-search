{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "#SPDX-License-Identifier: MIT-0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Open Search Serverless Index preparation\n",
        "\n",
        "In this notebook, we are creating a preparing the index on the OpenSearch collection and loading movie data. Once the data is loaded, we will compare search results across OpenSearch keyword search and semantic search capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install required libraries\n",
        "The following cell installs required python libraries specified in the 'requirements.txt' file. Execute this cell if you are starting from this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This cell installs the required libraries specified in the 'requirements.txt' file\n",
        "!pip install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load OpenSearch Collection details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have created an OpenSearch Serverless collection in the previous notebook. This notebook builds upon that by preparing an index and loading movie metadata into it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loading variables from previous notebook that will be needed for this one\n",
        "%store -r os_host\n",
        "%store -r collection_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next cell provides an alternative way to manually set the values of the `os_host`, `collection_name` variables, if needed. This can be useful if the stored values are not available or need to be overridden for a specific use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#alternatively you can manually set those values if you need\n",
        "#os_host = \"\"\n",
        "#collection_name = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load required libraries\n",
        "In the next cell, we import necessary Python libraries and modules. We also add the `llm_utils` module from the `../lib/src/utils/` directory to the system path. This module contains utility functions that we will use later in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "import json\n",
        "import pprint as pp\n",
        "import os\n",
        "import shutil\n",
        "import csv\n",
        "import time\n",
        "import pprint\n",
        "\n",
        "from opensearchpy import (\n",
        "    AWSV4SignerAuth\n",
        ")\n",
        "\n",
        "#adding our utils library to sys path\n",
        "import sys\n",
        "sys.path.append(\"../src/utils/\")\n",
        "import llm_utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Either set the region manually or use the default one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Manually set the region to deploy the openSearch serverless collection\n",
        "#REGION = 'us-east-1'\n",
        "\n",
        "#get default from boto3 session\n",
        "session = boto3.session.Session()\n",
        "# Get the default region from the session\n",
        "REGION = session.region_name\n",
        "\n",
        "print(f\"Region: {REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-requisites\n",
        "\n",
        "Prior to going through this workshop you need to create the Open search index and the required policies. We however need to add the execution role that you're using to run this notebook to the data access policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get current role/identity\n",
        "\n",
        "First we identify the current identity. it might be an execution role if you're using sagemaker or a user if you're using visual studio with the AWS CLI plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "identity_arn = \"\"\n",
        "\n",
        "try:\n",
        "    # Get the execution role ARN\n",
        "    identity_arn = sagemaker.get_execution_role()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"Not a sagemaker role, trying to retrieve the user identity\")\n",
        "    # Create an STS client\n",
        "    sts_client = boto3.client('sts')\n",
        "\n",
        "    # Get the caller identity\n",
        "    caller_identity = sts_client.get_caller_identity()\n",
        "    identity_arn = caller_identity['Arn']\n",
        "\n",
        "print(f\"Identity ARN:{identity_arn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the policy to allow the user or role to add a data access policy to the opensearch collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, an AWS Identity and Access Management (IAM) client is created using the boto3 library in Python. Then, a policy document is defined with a statement that allows all actions (\"aoss:*\") on all resources (\"Resource\": \"*\"). This policy document is used to create an IAM policy named \"AOSSAccessPolicy\" using the create_policy method of the IAM client. The created policy can be attached to IAM roles or users to grant them the specified permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an IAM client\n",
        "iam = boto3.client('iam')\n",
        "\n",
        "# Define the policy document\n",
        "policy_document = {\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": \"aoss:*\",\n",
        "            \"Resource\": \"*\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create the IAM policy\n",
        "aossAccessPolicy = iam.create_policy(\n",
        "    PolicyName='AOSSAccessPolicy',\n",
        "    PolicyDocument=json.dumps(policy_document)\n",
        ")\n",
        "\n",
        "\n",
        "aossAccessPolicyArn = aossAccessPolicy[\"Policy\"][\"Arn\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell, we wait for the policy to be created and then attach it to the current identity (user or role). This ensures that the current identity has the necessary permissions to add a data access policy to the OpenSearch Serverless collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, the script first waits for 10 seconds to ensure that the policy has been created. It then checks whether the provided identity ARN belongs to an IAM user or an IAM role. If it's a user, it attaches the newly created AOSS access policy to the user. If it's a role, it attaches the policy to the role instead. The code handles the NoSuchEntityException that could occur if the ARN doesn't correspond to a user, assuming it's a role in that case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#wait for the policy to be created\n",
        "time.sleep(10)\n",
        "\n",
        "# Check if the identity ARN is for a user or a role\n",
        "try:\n",
        "    # Try to get the user information\n",
        "    user = iam.get_user(UserName=identity_arn.split('/')[-1])\n",
        "    print(f\"The identity ARN '{identity_arn}' is for a user.\")\n",
        "\n",
        "    # Attach the policy to the user\n",
        "    iam.attach_user_policy(\n",
        "        UserName=user['User']['UserName'],\n",
        "        PolicyArn=aossAccessPolicyArn\n",
        "    )\n",
        "except iam.exceptions.NoSuchEntityException:\n",
        "    # If the identity ARN is not for a user, it must be for a role\n",
        "    print(f\"The identity ARN '{identity_arn}' is for a role.\")\n",
        "\n",
        "    # Attach the policy to the role\n",
        "    iam.attach_role_policy(\n",
        "        RoleName=identity_arn.split('/')[-1],\n",
        "        PolicyArn=aossAccessPolicyArn\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Access Policy creation for opensearch\n",
        "\n",
        "We create a new Data access policy to attach the notebook's execution role or user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code creates an OpenSearch Serverless client using boto3, defines a data access policy granting permissions to perform various operations on the \"semantic-search\" collection and its index, assigns the policy to a specified identity, and calls the `create_access_policy` method to create the data access policy with a given name, description, and policy document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an OpenSearch Serverless client\n",
        "opss_client = boto3.client('opensearchserverless')\n",
        "\n",
        "data_access_policy = json.dumps([\n",
        "      {\n",
        "        \"Rules\": [\n",
        "          {\n",
        "            \"Resource\": [\n",
        "              f\"collection/{collection_name}\"\n",
        "            ],\n",
        "            \"Permission\": [\n",
        "              \"aoss:CreateCollectionItems\",\n",
        "              \"aoss:DeleteCollectionItems\",\n",
        "              \"aoss:UpdateCollectionItems\",\n",
        "              \"aoss:DescribeCollectionItems\"\n",
        "            ],\n",
        "            \"ResourceType\": \"collection\"\n",
        "          },\n",
        "          {\n",
        "            \"Resource\": [\n",
        "              f\"index/{collection_name}/*\"\n",
        "            ],\n",
        "            \"Permission\": [\n",
        "              \"aoss:CreateIndex\",\n",
        "              \"aoss:DeleteIndex\",\n",
        "              \"aoss:UpdateIndex\",\n",
        "              \"aoss:DescribeIndex\",\n",
        "              \"aoss:ReadDocument\",\n",
        "              \"aoss:WriteDocument\"\n",
        "            ],\n",
        "            \"ResourceType\": \"index\"\n",
        "          }\n",
        "        ],\n",
        "        \"Principal\": [\n",
        "          identity_arn\n",
        "        ],\n",
        "        \"Description\": \"data-access-rule\"\n",
        "      }\n",
        "    ], indent=2)\n",
        "\n",
        "data_access_policy_name_nb = f\"{collection_name}-policy-notebook\"\n",
        "\n",
        "# Create the data access policy\n",
        "response = opss_client.create_access_policy(\n",
        "    description='Data access policy for semantic search collection',\n",
        "    name=data_access_policy_name_nb,\n",
        "    policy=str(data_access_policy),\n",
        "    type='data'\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create semantic search engine with Amazon OpenSearch Service Serverless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Opensearch data access policy update\n",
        "\n",
        "To access our opensearch serverless collection, we need to update its data policy update with the current user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code cell sets up an AWS OpenSearch Serverless client by assigning the service name, obtaining AWS credentials, creating an authentication object, and then using the LLMUtils module to create the client with the authentication object and the OpenSearch host."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#opensearch serverless service, aka aoss\n",
        "service = 'aoss'\n",
        "\n",
        "#get an Auth object to call aoss\n",
        "credentials = boto3.Session().get_credentials()\n",
        "auth = AWSV4SignerAuth(credentials, REGION, service)\n",
        "\n",
        "#LLMUtils.connect_to_aoss() can be found in lib/src/utils/ folder.\n",
        "aoss_client = llm_utils.connect_to_aoss(auth, os_host)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create OpenSearch index\n",
        "\n",
        "In the next few cells, we define the index name, data columns to be included, and the index configuration. We then create the index if it doesn't already exist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, an OpenSearch index named \"movies-index\" is defined. The code also specifies a list of data columns that will be added to this index, including information about movies such as their TMDB ID, original language, title, description, genres, release year, keywords, director, actors, popularity score, popularity score bins, average vote rating, and average vote rating bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#opensearch index name\n",
        "index_name = \"movies-index\"\n",
        "\n",
        "#data column to add to the index\n",
        "data_columns = ['tmdb_id', 'original_language', 'original_title', 'description', 'genres', 'year', 'keywords', 'director', 'actors', 'popularity', 'popularity_bins',\n",
        "                  'vote_average', 'vote_average_bins']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#index configuration. note that we're adding both text metadata as well as the vector_index property that will be storing our embedding for each title.\n",
        "# For additional information on the K-NN index configuration, please read the below documentation.\n",
        "#https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector/\n",
        "#https://opensearch.org/docs/latest/search-plugins/knn/knn-index/\n",
        "\n",
        "index_body = {\n",
        "  \"settings\": {\n",
        "    \"index\": {\n",
        "      'number_of_shards': 4,\n",
        "      \"number_of_replicas\": 0,\n",
        "      \"knn\": True,\n",
        "      \"knn.algo_param.ef_search\": 100\n",
        "    }\n",
        "  },\n",
        "  \"mappings\": {\n",
        "    \"properties\": {\n",
        "      \"tmdb_id\": {\"type\": \"integer\"},\n",
        "      \"original_language\": {\"type\": \"text\"},\n",
        "      \"original_title\": {\"type\": \"text\"},\n",
        "      \"description\": {\"type\": \"text\"},\n",
        "      \"genres\": {\"type\": \"text\"},\n",
        "      \"year\": {\"type\": \"integer\"},\n",
        "      \"keywords\": {\"type\": \"text\"},\n",
        "      \"director\": {\"type\": \"text\"},\n",
        "      \"actors\": {\"type\": \"text\"},\n",
        "      \"popularity\": {\"type\": \"float\"},\n",
        "      \"popularity_bins\": {\"type\": \"text\"},\n",
        "      \"vote_average\": {\"type\": \"float\"},\n",
        "      \"vote_average_bins\": {\"type\": \"text\"},\n",
        "      \"vector_index\": {\n",
        "        \"type\": \"knn_vector\",\n",
        "        \"dimension\": 1024, #if you use cohere: dimension of the embedding is 1024, for titan: 1536\n",
        "        \"method\": {\n",
        "          \"name\": \"hnsw\",\n",
        "          \"space_type\": \"l2\",\n",
        "          \"engine\": \"nmslib\",\n",
        "          \"parameters\": {\n",
        "            \"ef_construction\": 512,\n",
        "            \"m\": 16\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section of the code checks if the specified index already exists in OpenSearch. If the index does not exist, it creates a new index with the provided index body. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, it first retrieves a list of existing index names in the Elasticsearch cluster using the `get_alias` method of the `indices` client. It then checks if the specified `index_name` is not present in the list of existing indexes. If it's not present, it creates a new index with the given `index_name` and `index_body` configuration using the `create` method of the `indices` client. If the index already exists, it prints a message indicating that the index already exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get a list of the indexes already existing\n",
        "indexes = aoss_client.indices.get_alias(\"*\")\n",
        "indexes_list = list(indexes.keys())\n",
        "\n",
        "#check if index doesn't already exist and create it\n",
        "if index_name not in indexes_list:\n",
        "    print('Creating index:\\n')\n",
        "    create_response = aoss_client.indices.create(index_name, body=index_body)\n",
        "    print(create_response)\n",
        "else:\n",
        "    print(\"index already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we display information about the newly created index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code cell retrieves and displays details about a previously created OpenSearch index, including its mapping (fields and data types), settings (configuration options like shards and replicas), and any associated aliases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#display information on the index you just created\n",
        "\n",
        "# Get index mapping\n",
        "response = aoss_client.indices.get_mapping(index=index_name)\n",
        "pp.pprint(response) \n",
        "\n",
        "# Get index settings\n",
        "response = aoss_client.indices.get_settings(index=index_name)\n",
        "pp.pprint(response)\n",
        "\n",
        "# Get index aliases\n",
        "response = aoss_client.indices.get_alias(index=index_name) \n",
        "pp.pprint(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create embeddings from CSV file\n",
        "This section sets up the path to the CSV file containing movie metadata. It provides the option to use either a small dataset with 1,000 movies or a larger dataset with 45,000 movies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, a path to a CSV file containing movie metadata is being assigned to the variable `movies_data_path`. The commented line shows an alternative path for a larger dataset containing metadata for 45,000 movies. By default, the script is configured to use a smaller dataset with 1,000 movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#small dataset with 200 movies\n",
        "movies_data_path = \"../dataset/movies_metadata_small.csv\"\n",
        "\n",
        "#full dataset with 45K movies\n",
        "#movies_data_path = \"../dataset/movies_metadata_45K.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we get the Bedrock instances using the boto3 library, which will be used for generating embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get bedrock instances with boto3\n",
        "bedrock = boto3.client('bedrock')\n",
        "bedrock_client = boto3.client('bedrock-runtime')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code block creates a local folder where the embeddings will be generated. It checks if the folder already exists and cleans it up if necessary, or creates a new folder if it doesn't exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, the script checks if a directory named \"../tmp/embeddings\" exists. If it does, it deletes all files and subdirectories within that folder. After that, it recreates the \"../tmp/embeddings\" directory. This process ensures that the specified folder is empty before any new files are added to it, in preparation for generating and storing embeddings within that directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#local folder where embeddings will be generated\n",
        "embeddings_folder_path = \"../tmp/embeddings\"\n",
        "\n",
        "if os.path.exists(embeddings_folder_path):\n",
        "  print(\"Folder already exists, deleting contents\")\n",
        "  for filename in os.listdir(embeddings_folder_path):\n",
        "    file_path = os.path.join(embeddings_folder_path, filename)\n",
        "    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "        os.unlink(file_path)\n",
        "    elif os.path.isdir(file_path):\n",
        "        shutil.rmtree(file_path)\n",
        "  os.rmdir(embeddings_folder_path)\n",
        "\n",
        "print(\"Recreating folder\")  \n",
        "os.makedirs(embeddings_folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're now ready to generate the embeddings. expect it to take around 20min.\n",
        "\n",
        "Here, we generate the embeddings for each movie in the CSV file. This process involves reading the CSV file, generating embeddings using the Bedrock service, and writing the embeddings to JSON files in batches. The embeddings and movie metadata are combined into a single JSON document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, the script reads a CSV file containing movie data, processes each row to create a dictionary with metadata, generates vector embeddings for the metadata using the Bedrock library, combines the vector embeddings with the metadata, and writes the resulting JSON documents to separate files in batches of 100 records. The script handles writing the remaining records to a separate file if the total number of records is not divisible by the batch size. This process is key part of preparing data for semantic search application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limit the number of records to process in each block\n",
        "block_size = 100\n",
        "\n",
        "with open(movies_data_path) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    \n",
        "    #get header\n",
        "    header = next(csv_reader)\n",
        "    \n",
        "    #counter for line in csv file\n",
        "    line_num = 1\n",
        "\n",
        "    #document buffer\n",
        "    documents = []\n",
        "\n",
        "    for row in csv_reader:\n",
        "\n",
        "        #create dict with title's metadata using the header and the row values\n",
        "        title_metadata = dict()\n",
        "        for col in header:\n",
        "            title_metadata[col] = row[header.index(col)]\n",
        "\n",
        "        #generate embedding with Bedrock\n",
        "        vector_embedding = llm_utils.get_embeddings_from_text(json.dumps(title_metadata), \"cohere\", input_type=\"search_document\")\n",
        "\n",
        "        #merge vector and metadata\n",
        "        request_body_dict = dict()\n",
        "        request_body_dict['id'] = line_num\n",
        "        request_body_dict['vector_index'] = vector_embedding\n",
        "        request_body_dict = request_body_dict | title_metadata\n",
        "        \n",
        "        #dict to json string\n",
        "        request_body = json.dumps(request_body_dict)\n",
        "\n",
        "        #add to documents\n",
        "        documents.append(request_body)\n",
        "\n",
        "        #write down the json file every line_num\n",
        "        if line_num > 0 and line_num % block_size == 0:\n",
        "            print('writing file')\n",
        "            block_num = line_num // block_size\n",
        "            output_file_path = f\"{embeddings_folder_path}/embeddings_block{block_num}.json\"\n",
        "            with open(output_file_path, 'w') as output_file:\n",
        "                json.dump(documents, output_file, indent=2)\n",
        "                print(f\"Processed {len(documents)} records and saved to {output_file_path}\")\n",
        "                #reset documents buffer\n",
        "                documents = []\n",
        "\n",
        "        line_num += 1\n",
        "\n",
        "    if documents:\n",
        "        # Write the remaining documents to a file\n",
        "        output_file_path = f\"{embeddings_folder_path}/embeddings_block{block_num + 1}.json\"\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            json.dump(documents, output_file, indent=2)\n",
        "\n",
        "        print(f\"Processed {len(documents)} records and saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load embeddings in index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code cell defines the `format_data_for_bulk_import` function, which takes a list of JSON-encoded documents and returns a list of actions compatible with the bulk ingestion process of an OpenSearch index; it iterates through the input data, converts each JSON string to a dictionary, and constructs the appropriate action dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#format the data to match format expected by opensearch bulk ingest\n",
        "def format_data_for_bulk_import(data):\n",
        "    actions = []\n",
        "    for doc in data:\n",
        "        #str to dict\n",
        "        doc_dict = json.loads(doc)\n",
        "\n",
        "        #building the json format required for bulk index\n",
        "        actions.append({\n",
        "            \"_op_type\": \"index\",\n",
        "            \"_index\": index_name,\n",
        "            #\"_id\": doc_dict['id'],  #not allowed for index operation\n",
        "            \"_source\": {\n",
        "                \"vector_index\": doc_dict[\"vector_index\"],\n",
        "                \"tmdb_id\" : doc_dict['tmdb_id'],\n",
        "                \"original_language\" : doc_dict['original_language'],\n",
        "                \"original_title\" : doc_dict['original_title'],\n",
        "                \"description\" : doc_dict['description'],\n",
        "                \"genres\" : doc_dict['genres'],\n",
        "                \"year\" : doc_dict['year'],\n",
        "                \"keywords\" : doc_dict['keywords'],\n",
        "                \"director\" : doc_dict['director'],\n",
        "                \"actors\" : doc_dict['actors'],\n",
        "                \"popularity\" : doc_dict['popularity'],\n",
        "                \"popularity_bins\" : doc_dict['popularity_bins'],\n",
        "                \"vote_average\" : doc_dict['vote_average'],\n",
        "                \"vote_average_bins\" : doc_dict['vote_average_bins']\n",
        "            }\n",
        "        })\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we iterate through the JSON files containing the embeddings and movie metadata, format the data for bulk import, and use the OpenSearch bulk API to index the documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The code cell imports the `bulk` function from `opensearchpy.helpers`, reconnects to the OpenSearch service using authentication credentials and a host URL, iterates through JSON files in a directory, loads the JSON data, formats it for bulk import using `format_data_for_bulk_import`, prints which file is being indexed, uses the `bulk` function to insert the formatted documents into the specified OpenSearch index with `raise_on_exception=True`, and prints a summary of successfully indexed and failed documents for each file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opensearchpy.helpers import bulk\n",
        "\n",
        "#reconnect to avoid potential timeout of the session\n",
        "aoss_client = llm_utils.connect_to_aoss(auth, os_host)\n",
        "\n",
        "# Iterate through each JSON file\n",
        "for filename in os.listdir(embeddings_folder_path):\n",
        "    \n",
        "    file_path = os.path.join(embeddings_folder_path, filename)\n",
        "\n",
        "    # Load JSON file\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        #format data into actions\n",
        "        actions = format_data_for_bulk_import(data)\n",
        "\n",
        "        print(f\"Indexing {filename}\")\n",
        "\n",
        "        # Use the bulk API to insert documents from the file\n",
        "        success, failed = bulk(\n",
        "            aoss_client,\n",
        "            actions,\n",
        "            index=index_name, \n",
        "            raise_on_exception=True\n",
        "        )\n",
        "\n",
        "    print(f\"Indexed {success} documents successfully, {failed} documents failed for file: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After indexing, we check the number of documents in the index to ensure that the indexing process was successful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cell, it first introduces a delay of 30 seconds using `time.sleep(30)`. \n",
        "\n",
        "Subsequently, it prints the number of documents present in an index named `index_name` using the `aoss_client.count()` method. The purpose of this code is to verify the number of documents indexed, which is expected to be 200 if a small dataset has been successfully indexed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checking how many document we have in the index (might need a couple of refresh. you should see 200 if you've indexed the small dataset)\n",
        "time.sleep(40)\n",
        "print(f\"number of docs in index:{aoss_client.count(index=index_name)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search the index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using vector embeddings and KNN\n",
        "\n",
        "This section demonstrates how to search the index using vector embeddings and the k-nearest neighbors (KNN) algorithm. It generates an embedding for a given question, constructs a KNN query, and retrieves the top k most relevant documents from the index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code cell generates a vector embedding representation of the example question \"list horror movies that take place in nature\" using the Cohere language model, sets k=5 for the number of documents to retrieve, constructs a query to search for the k nearest neighbors to the question embedding vector in the specified vector index, and performs the search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#example question\n",
        "question = \"list horror movies that take place in nature\"\n",
        "\n",
        "#we generate the vectorised version of the question\n",
        "question_embedding = llm_utils.get_embeddings_from_text(question, \"cohere\", input_type=\"search_query\")\n",
        "\n",
        "#number of documents to retrieve\n",
        "k = 5\n",
        "\n",
        "query = {\n",
        "    \"size\": k,\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "        \"vector_index\": {\n",
        "            \"vector\": question_embedding,\n",
        "            \"k\": k\n",
        "        }\n",
        "        }\n",
        "    },\n",
        "    \"_source\": data_columns\n",
        "}\n",
        "\n",
        "search_response = aoss_client.search(body=query, index=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we extract the relevant information from the OpenSearch response and display the search results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code prints the embedding search response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract object from os response\n",
        "response = llm_utils.extract_response_from_os_response(search_response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the normal opensearch index (and not the embeddings)\n",
        "\n",
        "This section shows how to perform a regular text-based search on the OpenSearch index without using vector embeddings. It constructs a boolean query to search for movies with a specific title and an actor, sorts the results by popularity, and retrieves the top 10 documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = {\n",
        "    \"query\": {\n",
        "        \"bool\": {\n",
        "            \"must\": [\n",
        "                {\n",
        "                    \"match\": {\n",
        "                        \"original_title\": \"deadpool\"\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"match_phrase\": {\n",
        "                        \"actors\": \"Ryan Reynolds\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    \"size\": 10,\n",
        "    \"sort\": [\n",
        "        {\n",
        "            \"popularity\": {\n",
        "                \"order\": \"desc\"\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "search_response = aoss_client.search(body=query, index=index_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we extract the relevant information from the OpenSearch response, remove the vector_index field (since it's not used in this scenario), and display the search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_extracted = llm_utils.extract_response_from_os_response(search_response)\n",
        "#removing the vector_index as we are not using it in that scenario\n",
        "if len(response_extracted) > 0:\n",
        "    for i in range(len(response_extracted)):\n",
        "        response_extracted[i].pop(\"vector_index\")\n",
        "\n",
        "response_extracted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hybrid Search - the best of both world?\n",
        "\n",
        "Hybrid search is a technique that combines traditional lexical search and semantic search to improve search relevance. \n",
        "\n",
        "The motivation behind this approach is that different queries perform better with either lexical or semantic search. By combining these two methods, search performance can be enhanced. The challenge in implementing hybrid search lies in normalizing the similarity scores produced by the two types of search, as they use different scales. \n",
        "\n",
        "<br/>\n",
        "Various benchmarks show a significant performance improvement using Hybrid search versus lexical or semantic only. \n",
        "Example: https://opensearch.org/blog/hybrid-search/\n",
        "\n",
        "<i>\"hybrid search improves the result quality by 8â€“12% compared to keyword search and by 15% compared to natural language search\"</i>\n",
        "\n",
        "<br/>\n",
        "Depending on the underlying Vector DB/store you are using, the solution will differ. \n",
        "\n",
        "<b>With non serverless deployment of Amazon OpenSearch service</b> using features released in September 2023 as part of OpenSearch 2.10, you can do that relatively easily using a \"search pipeline\" that includes a normalization processor. The pipeline runs at search time and normalizes the scores to a common scale, enabling meaningful combination. The process involves using min-max normalization and harmonic mean as the normalization and combination techniques, respectively. Additionally, the weights assigned to the query clauses can be adjusted to balance the influence of lexical and semantic search.\n",
        "\n",
        "<b>Amazon OpenSearch Serverless</b> does not support this feature yet so you need to develop the feature outside of Opensearch for the moment.\n",
        "\n",
        "Same for other vectorDBs including Amazon RDS for <b>PostgreSQL using pgvector</b> for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RFF - Reciprocal Rank Fusion\n",
        "\n",
        "There are various strategies and algorithm to merge 2 lists of results. \n",
        "\n",
        "The Reciprocal Rank Fusion (RRF) is a sophisticated algorithm that combines different sets of results, each with its own relevance scores, into a single unified set of results. One of the main benefits of RRF is that it can produce high-quality results without requiring any adjustments or tuning. Additionally, RRF does not require the relevance scores from different sources to be related or similar in nature.\n",
        "\n",
        "How does it work?\n",
        "\n",
        "The Reciprocal Rank Fusion (RRF) technique works by gathering search results from multiple different approaches, assigning each document in the results a score based on its reciprocal rank, and then combining these scores to produce a new ranking. The core idea behind this method is that documents that consistently appear at higher ranks across various search strategies are more likely to be relevant, and therefore, should receive a higher ranking in the consolidated search result.\n",
        "\n",
        "Algorithm explained here: \n",
        "https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#merge 2 lists of movies and remove duplicates\n",
        "def merge_remove_duplicates(list1, list2):\n",
        "    # Create an empty dictionary to store unique entries\n",
        "    unique_entries = {}\n",
        "\n",
        "    # Iterate over the first list\n",
        "    for entry in list1:\n",
        "        tmdb_id = entry['tmdb_id']\n",
        "        unique_entries[tmdb_id] = entry\n",
        "\n",
        "    # Iterate over the second list\n",
        "    for entry in list2:\n",
        "        tmdb_id = entry['tmdb_id']\n",
        "        if tmdb_id not in unique_entries:\n",
        "            unique_entries[tmdb_id] = entry\n",
        "\n",
        "    # Convert the dictionary back to a list\n",
        "    merged_list = list(unique_entries.values())\n",
        "\n",
        "    return merged_list\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(list1, list2):\n",
        "    # Create a dictionary to store ranks\n",
        "    ranks = {}\n",
        "\n",
        "    max_size = max(len(list1), len(list2))\n",
        "    \n",
        "    # Assign ranks to items in list1\n",
        "    for rank, item in enumerate(list1, start=1):\n",
        "        tmdb_id = item['tmdb_id']\n",
        "        ranks.setdefault(tmdb_id, []).append(rank)\n",
        "    \n",
        "    # Assign ranks to items in list2\n",
        "    for rank, item in enumerate(list2, start=1):\n",
        "        tmdb_id = item['tmdb_id']\n",
        "        ranks.setdefault(tmdb_id, []).append(rank)\n",
        "    \n",
        "    #ranks will look like: {'168259': [1, 5], '460846': [2, 12], '294254': [3]}\n",
        "    \n",
        "    # Calculate RRF scores\n",
        "    # example: for '168259': [1, 5], the RFF score is 1/1 + 1/5 = 1.2\n",
        "    rrf_scores = {}\n",
        "    for tmdb_id, ranks_list in ranks.items():\n",
        "        rrf_score = sum(1 / rank for rank in ranks_list)\n",
        "        rrf_scores[tmdb_id] = rrf_score\n",
        "    \n",
        "    merged_list = merge_remove_duplicates(list1, list2)\n",
        "    for item in merged_list:\n",
        "        tmdb_id = item['tmdb_id']\n",
        "        item['RFF_score'] = rrf_scores.get(tmdb_id, 0)\n",
        "\n",
        "    sorted_merged_list = sorted(merged_list, key=lambda x: x['RFF_score'], reverse=True)\n",
        "\n",
        "    return sorted_merged_list[:max_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate a dataset of semantic and lexical queries about genre as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Genres to be used as query for the API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "genres = [\"Mystery\" , \"Science Fiction\", \"Thriller\", \"Adventure\", \"Horror\", \"Comedy\", \"Drama\", \"Crime\", \"Western\", \"Romance\", \"Family\", \"Animation\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling APIS for lexical, semantic and generating Hybrid search results and storing that aside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aoss_client = llm_utils.connect_to_aoss(auth, os_host)\n",
        "\n",
        "#number of items to be returned\n",
        "k = 10\n",
        "\n",
        "lexical_queries_results = dict()\n",
        "semantic_queries_results = dict()\n",
        "hybrid_queries_results = dict()\n",
        "\n",
        "for genre in genres:\n",
        "\n",
        "    #---------- lexical search -----------\n",
        "    query_lexical = {\n",
        "        \"query\": {\n",
        "        \"match\": {\n",
        "        \"genres\": genre\n",
        "        }\n",
        "    },\n",
        "        \"size\": k,\n",
        "    }\n",
        "    search_response = aoss_client.search(body=query_lexical, index=index_name)\n",
        "    lexical_response = llm_utils.extract_response_from_os_response(search_response)\n",
        "    #removing the vector_index as we are not using it in that scenario\n",
        "    if len(lexical_response) > 0:\n",
        "        for i in range(len(lexical_response)):\n",
        "            lexical_response[i].pop(\"vector_index\")\n",
        "    \n",
        "    lexical_queries_results[genre] = lexical_response\n",
        "\n",
        "    #-------- semantic search -----------\n",
        "    #we generate the vectorised version of the question\n",
        "    question_embedding = llm_utils.get_embeddings_from_text(genre, \"cohere\", input_type=\"search_query\")\n",
        "    query_semantic = {\n",
        "        \"size\": k,\n",
        "        \"query\": {\n",
        "            \"knn\": {\n",
        "            \"vector_index\": {\n",
        "                \"vector\": question_embedding,\n",
        "                \"k\": k\n",
        "            }\n",
        "            }\n",
        "        },\n",
        "        \"_source\": data_columns\n",
        "    }\n",
        "    response = aoss_client.search(body=query_semantic, index=index_name)\n",
        "    #extract object from os response\n",
        "    semantic_response = llm_utils.extract_response_from_os_response(response)\n",
        "    semantic_queries_results[genre] = semantic_response\n",
        "\n",
        "    #---------- Hybrid search fusion with RRF -----------\n",
        "    RFF_response  = reciprocal_rank_fusion(lexical_response, semantic_response)\n",
        "    hybrid_queries_results[genre] = RFF_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation of our search result\n",
        "Instead of manually building our evaluation dataset, let's try to get Claude3 to do it for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "small_dataset_df = pd.read_csv(\"../dataset/movies_metadata_small.csv\")\n",
        "print(f\"dataset size:{small_dataset_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_dataset_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We build a prompt for Claude3 to select the \"best\" movies for us based on common criteria like awards, critics or cultural impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preselection_size = k*2\n",
        "\n",
        "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Your task is to extract the \"best\" <number>{k}</number> movies from the list provided in the <documents> tag in relation to this specific <genre>{genre}</genre>.\n",
        "\n",
        "SUPER IMPORTANT: DO NOT COME UP WITH MOVIES FROM YOUR OWN KNOWLEDGE!! you need to extract the list from the provided documents in <documents> tag only.\n",
        "\n",
        "Read carefully through the list of movies before responding. \n",
        "\n",
        "When evaluating which are the best movies consider those criteria: cultural impact and influence, critics, awards.\n",
        "\n",
        "Extract from the list in <documents> tag a pre-selection of <number>{preselection_size}</number> movies and write them in <thinking> tag before responding in <answer> tag.\n",
        "\n",
        "Output your final selection of <number>{k}</number> in a well JSON format as show in the <example> tag. Note that the tmdb_id is an integer, not a string.\n",
        "\n",
        "<example>\n",
        "    <thinking>[{\"tmbd_id\":2343,\"original_title\":\"Rocky\"}, {\"tmbd_id\":23434,\"original_title\":\"Batman\"}, {\"tmbd_id\":43232,\"original_title\":\"Rambo\"}]</thinking>\n",
        "    <answer>[{\"tmbd_id\":2343,\"original_title\":\"Rocky\"}, {\"tmbd_id\":43232,\"original_title\":\"Rambo\"}]</answer>\n",
        "<example>\n",
        "\n",
        "<documents>\n",
        "{movies}\n",
        "</documents>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define few utils functions to help us format the dataframe into json that we can then include into the system prompt as part of the LLM API call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "#Convert a DataFrame to a list of JSON-formatted objects.\n",
        "def df_to_json(df):\n",
        "    json_list = []\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        json_dict = row.to_dict()\n",
        "        json_list.append(json_dict)\n",
        "    \n",
        "    return json_list\n",
        "\n",
        "#Format a list of JSON objects into a string.\n",
        "def format_json_list(json_list):\n",
        "    \n",
        "    formatted_string = \"\"\n",
        "    for json_obj in json_list:\n",
        "        formatted_string += \"{\\n\"\n",
        "        for key, value in json_obj.items():\n",
        "            formatted_string += f\"  '{key}': '{value}',\\n\"\n",
        "        formatted_string = formatted_string.rstrip(\",\\n\") + \"\\n},\\n\"\n",
        "    formatted_string = formatted_string.rstrip(\",\\n\")\n",
        "    return formatted_string\n",
        "\n",
        "#Extract the string content from within <answer> tags in the given text.\n",
        "def extract_answer_text(text):\n",
        "    pattern = r'<answer>(.*?)</answer>'\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we call the API to generate our list of best movies per genre. It should take around 3min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#we prepare a dictionary to check the llm outputs.\n",
        "dataset_dict = dict()\n",
        "for idx, row in enumerate(small_dataset_df[['tmdb_id', 'original_title']].iterrows(), start=1):\n",
        "    dataset_dict[row[1][\"tmdb_id\"]] = row[1][\"original_title\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#llm generated relevant selection\n",
        "llm_generated_relevant_data = dict()\n",
        "\n",
        "for genre in genres:\n",
        "    #get filtered df based on genre.\n",
        "    genre_filtered_df = small_dataset_df[small_dataset_df[\"genres\"].str.contains(genre)]\n",
        "    \n",
        "    #convert df to json format.\n",
        "    json_docs = df_to_json(genre_filtered_df)\n",
        "    str_json_docs = format_json_list(json_docs)\n",
        "    \n",
        "    #format prompt\n",
        "    formated_system_prompt = prompt.replace(\"{movies}\", str_json_docs).replace(\"{k}\", str(k)).replace(\"{preselection_size}\", str(preselection_size)).replace(\"{genre}\", genre)\n",
        "\n",
        "    response = llm_utils.invoke_anthropic_claude(formated_system_prompt, \n",
        "                            system_prompt = \"\",\n",
        "                            max_tokens=4096, \n",
        "                            temperature=0, \n",
        "                            top_k=250, \n",
        "                            top_p=0.999,\n",
        "                            modelId=model_id,\n",
        "                            anthropic_version=\"bedrock-2023-05-31\",\n",
        "                            debug=False)\n",
        "    try:\n",
        "        json_response = json.loads(extract_answer_text(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error while parsing json response: {e}\")\n",
        "        print(f\"response:{response}\")\n",
        "        continue\n",
        "\n",
        "    #check that the llm output is correct\n",
        "    for item in json_response:\n",
        "        try:\n",
        "            is_valid = dataset_dict[item[\"tmdb_id\"]] == item[\"original_title\"]\n",
        "            if not is_valid:\n",
        "                print(f\"Error for item:{item}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Exception for item:{item}, {e}\")\n",
        "\n",
        "    llm_generated_relevant_data[genre] = json_response\n",
        "\n",
        "    print(f\"Data generated for {genre} genre\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's explore manually the suggestions made by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[title[\"original_title\"] for title in llm_generated_relevant_data['Mystery']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[title[\"original_title\"] for title in llm_generated_relevant_data['Drama']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision\n",
        "To calculate the precision of a search result, we need to determine the ratio of relevant items in the list to be evaluated compared to the total number of items in that list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_precision(relevant_dict, evaluated_dict):\n",
        "    precision_array = []\n",
        "    for index, key in enumerate(evaluated_dict.keys()):\n",
        "        relevant_count = 0\n",
        "        relevant_ids = [item[\"tmdb_id\"] for item in relevant_dict[key]]\n",
        "\n",
        "        for item in evaluated_dict[key]:\n",
        "            if int(item[\"tmdb_id\"]) in relevant_ids:\n",
        "                relevant_count += 1\n",
        "\n",
        "        precision = relevant_count / len(evaluated_dict[key]) if evaluated_dict[key] else 0\n",
        "        #print(f\"{key}: {precision}\")\n",
        "        precision_array.append(precision)\n",
        "    \n",
        "    return sum(precision_array)/len(precision_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Precision for semantic search only:{calculate_precision(llm_generated_relevant_data, semantic_queries_results)}\")\n",
        "print(f\"Precision for lexical search only:{calculate_precision(llm_generated_relevant_data, lexical_queries_results)}\")\n",
        "print(f\"Precision for hybrid search:{calculate_precision(llm_generated_relevant_data, hybrid_queries_results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Reciprocal Rank (MRR)\n",
        "\n",
        "The Mean Reciprocal Rank (MRR) is a metric used to evaluate the performance of a ranking system, such as a search engine or a recommendation system. It measures the average reciprocal rank of the first relevant item in the ranked list.\n",
        "\n",
        "Here's how the MRR algorithm works:\n",
        "\n",
        "1. For each query (or search term), we have a list of relevant items and a list of items to be evaluated (the ranked list returned by the system).\n",
        "2. We iterate through the ranked list and find the position (rank) of the first relevant item.\n",
        "3. If no relevant item is found, the reciprocal rank is considered 0.\n",
        "4. Otherwise, the reciprocal rank is calculated as 1 / rank, where rank is the position of the first relevant item.\n",
        "5. The MRR is the average of the reciprocal ranks across all queries.\n",
        "\n",
        "A higher MRR value indicates better ranking performance, with a maximum value of 1 when the first item in the ranked list is always relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mrr(relevant_dict, ranked_dict):\n",
        "    total_queries = len(relevant_dict)\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for _, key in enumerate(ranked_dict.keys()):\n",
        "        #get relevant ids for the current genre\n",
        "        relevant_ids = set(item[\"tmdb_id\"] for item in relevant_dict[key])\n",
        "\n",
        "        for rank, item in enumerate(ranked_dict[key], start=1):\n",
        "            if int(item[\"tmdb_id\"]) in relevant_ids:\n",
        "                reciprocal_ranks.append(1 / rank)\n",
        "                break\n",
        "\n",
        "    if not reciprocal_ranks:\n",
        "        return 0.0\n",
        "\n",
        "    mrr = sum(reciprocal_ranks) / total_queries\n",
        "\n",
        "    return mrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"MRR for semantic search only:{calculate_mrr(llm_generated_relevant_data, semantic_queries_results)}\")\n",
        "print(f\"MRR for lexical search only:{calculate_mrr(llm_generated_relevant_data, lexical_queries_results)}\")\n",
        "print(f\"MRR for hybrid search:{calculate_mrr(llm_generated_relevant_data, hybrid_queries_results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "You would have noticed that we get better result for lexical search compared to semantic search results.\n",
        "Is is expected considering the type of query we've been using. Assuming that you have accurate metadata which is the case for this dataset, a lexical search will be more efficient than semantic search.\n",
        "If we use a more complex query like \"scifi either in space or another planet movies\", Semantic search will have the upper hand.\n",
        "\n",
        "With that in mind, we will apply according strategy in the next part of the workshop and route queries to either semantic or lexical search to maximise performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we store some variables index_name for use in subsequent notebooks in the workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%store index_name\n",
        "%store identity_arn\n",
        "%store aossAccessPolicyArn\n",
        "%store data_access_policy_name_nb"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "local_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
